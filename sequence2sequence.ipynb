{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49432221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "571dcc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83da3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상대경로로 코드를 짜려다보니 다운받아서 압축해재하는데만 이만큼의 코드가 필요함\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "url = 'http://www.manythings.org/anki/fra-eng.zip'\n",
    "filename = 'fra-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, filename) # 이런식으로 할수도 있구나\n",
    "\n",
    "with http.request('GET', url, preload_content = False) as r, open(zipfilename, 'wb') as out_file:\n",
    "    shutil.copyfileobj(r, out_file)\n",
    "\n",
    "# r = http.request('GET', url, preload_content = False)\n",
    "\n",
    "# out_file = open(zipfilename, 'wb')\n",
    "# shutil.copyfileobj(r, out_file)\n",
    "# 그냥 직접 다운받아서 open 하면 안돼? -> 안돼 상대경로로 잡으려고 일부러 이렇게 한거야\n",
    "\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)\n",
    "# zip_ref = zipfile.ZipFile(zipfilename, 'r')\n",
    "# zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4299d2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189986"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = pd.read_csv('fra.txt', names = ['src', 'tar', 'lic'], sep = '\\t')\n",
    "del lines['lic']\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8896655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27526</th>\n",
       "      <td>Can I do that here?</td>\n",
       "      <td>Est-ce que je peux faire ça ici ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59932</th>\n",
       "      <td>Are you blackmailing me?</td>\n",
       "      <td>Exercez-vous un chantage à mon égard ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22242</th>\n",
       "      <td>Do you want a car?</td>\n",
       "      <td>Tu veux une voiture ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5357</th>\n",
       "      <td>It's special.</td>\n",
       "      <td>C'est spécial.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56048</th>\n",
       "      <td>I've got some problems.</td>\n",
       "      <td>J'ai des problèmes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>It was quite funny.</td>\n",
       "      <td>Ce fut assez amusant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56983</th>\n",
       "      <td>She sang as she walked.</td>\n",
       "      <td>Elle chantait en marchant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38766</th>\n",
       "      <td>Will you eat dinner?</td>\n",
       "      <td>Déjeuneras-tu ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>I love it.</td>\n",
       "      <td>J'adore ça !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32847</th>\n",
       "      <td>You should turn in.</td>\n",
       "      <td>Tu devrais te rendre.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            src                                     tar\n",
       "27526       Can I do that here?       Est-ce que je peux faire ça ici ?\n",
       "59932  Are you blackmailing me?  Exercez-vous un chantage à mon égard ?\n",
       "22242        Do you want a car?                   Tu veux une voiture ?\n",
       "5357              It's special.                          C'est spécial.\n",
       "56048   I've got some problems.                     J'ai des problèmes.\n",
       "29995       It was quite funny.                   Ce fut assez amusant.\n",
       "56983   She sang as she walked.              Elle chantait en marchant.\n",
       "38766      Will you eat dinner?                         Déjeuneras-tu ?\n",
       "1035                 I love it.                            J'adore ça !\n",
       "32847       You should turn in.                   Tu devrais te rendre."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines.loc[:, 'src':'tar']\n",
    "# loc : label을 이용하여 행에 \"접근\"&\"인덱싱\"하는 메서드\n",
    "lines = lines[0:60000]\n",
    "lines.sample(10) #상위 10개가 아니라 랜덤으로 10개를 말함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de5fba11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18669</th>\n",
       "      <td>I'll beat you up!</td>\n",
       "      <td>\\t Je vais te cogner ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57369</th>\n",
       "      <td>The boy is very honest.</td>\n",
       "      <td>\\t C'est un garçon très honnête. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54895</th>\n",
       "      <td>I hope we can fix that.</td>\n",
       "      <td>\\t J'espère que nous pouvons arranger ça. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21679</th>\n",
       "      <td>You're not alone.</td>\n",
       "      <td>\\t Tu n'es pas seule. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33757</th>\n",
       "      <td>Don't take the bait.</td>\n",
       "      <td>\\t Ne mords pas à l'appât. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20317</th>\n",
       "      <td>Tom glanced away.</td>\n",
       "      <td>\\t Tom a détourné le regard. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18650</th>\n",
       "      <td>I worry too much.</td>\n",
       "      <td>\\t Je me fais trop de souci. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20760</th>\n",
       "      <td>Watch yourselves!</td>\n",
       "      <td>\\t Faites attention à vous ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21588</th>\n",
       "      <td>You're all alone.</td>\n",
       "      <td>\\t Vous êtes toutes seules. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8468</th>\n",
       "      <td>Tie your shoe.</td>\n",
       "      <td>\\t Attache ton lacet de chaussure. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           src                                           tar\n",
       "18669        I'll beat you up!                     \\t Je vais te cogner ! \\n\n",
       "57369  The boy is very honest.           \\t C'est un garçon très honnête. \\n\n",
       "54895  I hope we can fix that.  \\t J'espère que nous pouvons arranger ça. \\n\n",
       "21679        You're not alone.                      \\t Tu n'es pas seule. \\n\n",
       "33757     Don't take the bait.                 \\t Ne mords pas à l'appât. \\n\n",
       "20317        Tom glanced away.               \\t Tom a détourné le regard. \\n\n",
       "18650        I worry too much.               \\t Je me fais trop de souci. \\n\n",
       "20760        Watch yourselves!               \\t Faites attention à vous ! \\n\n",
       "21588        You're all alone.                \\t Vous êtes toutes seules. \\n\n",
       "8468            Tie your shoe.         \\t Attache ton lacet de chaussure. \\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x: '\\t ' + x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6e7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자 집합 구축(토큰 단위가 단어가 아닌 글자이기 때문)\n",
    "\n",
    "src_vocab = set() # set()은 집합 생성 함수 {3,4,5} 순서도 없고, 중복도 불가능한 바로 그 집합\n",
    "for line in lines.src:\n",
    "    for char in line:\n",
    "        src_vocab.add(char)\n",
    "        \n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d65ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab) + 1\n",
    "tar_vocab_size = len(tar_vocab) + 1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e42f7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스를 부여해서 일부만 출력하고 싶은데 집합 상태로는 순서가 없기때문에 일부만 못해 그래서!\n",
    "src_vocab = sorted(list(src_vocab)) \n",
    "tar_vocab = sorted(list(tar_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f078850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w']\n"
     ]
    }
   ],
   "source": [
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f483ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, '’': 77, '€': 78}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, '\\u2009': 100, '\\u200b': 101, '‘': 102, '’': 103, '\\u202f': 104}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "# 근데 왜 dict() 안에 [] 중괄호로 감쌌을까 중괄호 없으면 안되나?\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0532ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10], [31, 58, 10]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in lines.src:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(src_to_index[w]) #dict[key] = value\n",
    "    encoder_input.append(temp_X)\n",
    "print(encoder_input[:5])\n",
    "#char마다 정수인코딩 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c413a671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 48, 53, 3, 4, 3, 2], [1, 3, 39, 53, 70, 55, 60, 57, 14, 3, 2], [1, 3, 28, 67, 73, 59, 57, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 14, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d64b31ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 48, 53, 3, 4, 3, 2], [3, 39, 53, 70, 55, 60, 57, 14, 3, 2], [3, 28, 67, 73, 59, 57, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 14, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    t = 0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        if t > 0:\n",
    "            temp_X.append(tar_to_index[w])\n",
    "        t = t + 1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])\n",
    "# 왜 이렇게 하는거야? 그냥 decoder_input에서 모든 리스트들마다 첫번째걸 삭제하면 되는거 아니야?\n",
    "# 아 맞다 파이썬은 리스트 원소 삭제 기능이 없었지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15a2b742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "# 패딩 해보자\n",
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd20e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_src_len, padding='post')\n",
    "# padding = 'pre' : 앞쪽에 패딩, 'post' : 뒷쪽에 패딩\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_tar_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80aa4947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫인코딩 : 단위가 char 이기 때문에 임베딩은 따로 필요 없음\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "354ac3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교사 강요(teacher forcing)를 통한 seq2seq 기계 번역기 훈련\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e309251d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input[6] # 가로는 padding한 max_src_len 24일거고, 세로는 char갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "340ef3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 미묘하게 다름 input's' 임\n",
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state = True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_outputs : hidden state\n",
    "# state_h : last hidden state\n",
    "# state_c : last cell state\n",
    "encoder_states = [state_h, state_c]\n",
    "# 마지막 hiddenstate, cellstate 두개가 context임 둘다 디코더로 전달함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77ff6b",
   "metadata": {},
   "source": [
    "# 케라스 API  \n",
    "  \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(3, input_dim=4, activation='softmax'))\n",
    "\n",
    "이것처럼 Sequential() API는 단순히 층을 쌓는 것만으로 기본적인 모델을 만들수 있지만  \n",
    "쌓는게 전부가 아닌 복잡한 신경망들은 Sequential API 만으로는 구현할수 없음  \n",
    "더 복작한 신경망들은 Functional API의 Model을 이용해서 하나하나 정의내려줘야함  \n",
    "  \n",
    "https://wikidocs.net/38861  \n",
    "위의 링크 반드시 시도해볼것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62bdf1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
    "# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 함\n",
    "\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation = 'softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# Model(input, output)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8eb403b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 248s 5ms/sample - loss: 0.7463 - val_loss: 0.6720\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 258s 5ms/sample - loss: 0.4624 - val_loss: 0.5443\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 276s 6ms/sample - loss: 0.3836 - val_loss: 0.4739\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 291s 6ms/sample - loss: 0.3402 - val_loss: 0.4333\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 292s 6ms/sample - loss: 0.3116 - val_loss: 0.4079\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 289s 6ms/sample - loss: 0.2910 - val_loss: 0.3924\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 287s 6ms/sample - loss: 0.2751 - val_loss: 0.3801\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 283s 6ms/sample - loss: 0.2621 - val_loss: 0.3715\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 278s 6ms/sample - loss: 0.2513 - val_loss: 0.3647\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 279s 6ms/sample - loss: 0.2420 - val_loss: 0.3589\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 282s 6ms/sample - loss: 0.2338 - val_loss: 0.3551\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 290s 6ms/sample - loss: 0.2266 - val_loss: 0.3540\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 298s 6ms/sample - loss: 0.2199 - val_loss: 0.3507\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 280s 6ms/sample - loss: 0.2141 - val_loss: 0.3492\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 279s 6ms/sample - loss: 0.2084 - val_loss: 0.3487\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 276s 6ms/sample - loss: 0.2032 - val_loss: 0.3490\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 274s 6ms/sample - loss: 0.1984 - val_loss: 0.3497\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 271s 6ms/sample - loss: 0.1939 - val_loss: 0.3492\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 270s 6ms/sample - loss: 0.1898 - val_loss: 0.3506\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 272s 6ms/sample - loss: 0.1857 - val_loss: 0.3504\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 270s 6ms/sample - loss: 0.1820 - val_loss: 0.3529\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 271s 6ms/sample - loss: 0.1784 - val_loss: 0.3550\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 270s 6ms/sample - loss: 0.1750 - val_loss: 0.3556\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 267s 6ms/sample - loss: 0.1718 - val_loss: 0.3569\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 264s 5ms/sample - loss: 0.1687 - val_loss: 0.3608\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 271s 6ms/sample - loss: 0.1658 - val_loss: 0.3615\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 263s 5ms/sample - loss: 0.1631 - val_loss: 0.3645\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 266s 6ms/sample - loss: 0.1603 - val_loss: 0.3669\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 266s 6ms/sample - loss: 0.1579 - val_loss: 0.3682\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 267s 6ms/sample - loss: 0.1553 - val_loss: 0.3724\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 266s 6ms/sample - loss: 0.1530 - val_loss: 0.3728\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 267s 6ms/sample - loss: 0.1508 - val_loss: 0.3754\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 274s 6ms/sample - loss: 0.1487 - val_loss: 0.3780\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 280s 6ms/sample - loss: 0.1466 - val_loss: 0.3796\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 272s 6ms/sample - loss: 0.1446 - val_loss: 0.3835\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 265s 6ms/sample - loss: 0.1427 - val_loss: 0.3860\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 260s 5ms/sample - loss: 0.1409 - val_loss: 0.3876\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 254s 5ms/sample - loss: 0.1390 - val_loss: 0.3919\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 267s 6ms/sample - loss: 0.1373 - val_loss: 0.3966\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 278s 6ms/sample - loss: 0.1357 - val_loss: 0.3964\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 287s 6ms/sample - loss: 0.1342 - val_loss: 0.3974\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 293s 6ms/sample - loss: 0.1326 - val_loss: 0.4005\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 296s 6ms/sample - loss: 0.1312 - val_loss: 0.4043\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 294s 6ms/sample - loss: 0.1296 - val_loss: 0.4072\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 294s 6ms/sample - loss: 0.1283 - val_loss: 0.4110\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 295s 6ms/sample - loss: 0.1269 - val_loss: 0.4116\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 291s 6ms/sample - loss: 0.1256 - val_loss: 0.4159\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 291s 6ms/sample - loss: 0.1242 - val_loss: 0.4189\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 291s 6ms/sample - loss: 0.1232 - val_loss: 0.4197\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 286s 6ms/sample - loss: 0.1219 - val_loss: 0.4243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f92ba0f4650>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size = 64, epochs = 50, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bd6b98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 79)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 105)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 344064      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  370688      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 105)    26985       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 741,737\n",
      "Trainable params: 741,737\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "model.save('seq2seq_fraeng2.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83839efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 불러오기\n",
    "# from tensorflow.python.keras.models import load_model\n",
    "# model = load_model('seq2seq_fraeng2.h5')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8643e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"253pt\" viewBox=\"0.00 0.00 605.50 304.00\" width=\"505pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 300)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-300 601.5,-300 601.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140268164709392 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140268164709392</title>\n",
       "<polygon fill=\"none\" points=\"25,-249.5 25,-295.5 286,-295.5 286,-249.5 25,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"89.5\" y=\"-268.8\">input_1: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"154,-249.5 154,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"154,-272.5 210,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"210,-249.5 210,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"248\" y=\"-280.3\">[(?, ?, 79)]</text>\n",
       "<polyline fill=\"none\" points=\"210,-272.5 286,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"248\" y=\"-257.3\">[(?, ?, 79)]</text>\n",
       "</g>\n",
       "<!-- 140268165557264 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140268165557264</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 311,-212.5 311,-166.5 0,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"42.5\" y=\"-185.8\">lstm: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"85,-166.5 85,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"85,-189.5 141,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"141,-166.5 141,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"226\" y=\"-197.3\">(?, ?, 79)</text>\n",
       "<polyline fill=\"none\" points=\"141,-189.5 311,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"226\" y=\"-174.3\">[(?, 256), (?, 256), (?, 256)]</text>\n",
       "</g>\n",
       "<!-- 140268164709392&#45;&gt;140268165557264 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140268164709392-&gt;140268165557264</title>\n",
       "<path d=\"M155.5,-249.3799C155.5,-241.1745 155.5,-231.7679 155.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"159.0001,-222.784 155.5,-212.784 152.0001,-222.784 159.0001,-222.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140268166612688 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140268166612688</title>\n",
       "<polygon fill=\"none\" points=\"329.5,-166.5 329.5,-212.5 597.5,-212.5 597.5,-166.5 329.5,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"394\" y=\"-185.8\">input_2: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"458.5,-166.5 458.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"486.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"458.5,-189.5 514.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"486.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"514.5,-166.5 514.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"556\" y=\"-197.3\">[(?, ?, 105)]</text>\n",
       "<polyline fill=\"none\" points=\"514.5,-189.5 597.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"556\" y=\"-174.3\">[(?, ?, 105)]</text>\n",
       "</g>\n",
       "<!-- 140268166613776 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140268166613776</title>\n",
       "<polygon fill=\"none\" points=\"140,-83.5 140,-129.5 479,-129.5 479,-83.5 140,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189.5\" y=\"-102.8\">lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"239,-83.5 239,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"239,-106.5 295,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"295,-83.5 295,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387\" y=\"-114.3\">[(?, ?, 105), (?, 256), (?, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"295,-106.5 479,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387\" y=\"-91.3\">[(?, ?, 256), (?, 256), (?, 256)]</text>\n",
       "</g>\n",
       "<!-- 140268166612688&#45;&gt;140268166613776 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140268166612688-&gt;140268166613776</title>\n",
       "<path d=\"M420.6024,-166.3799C402.2398,-156.4832 380.6347,-144.8388 361.3822,-134.4625\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"362.8978,-131.3034 352.4344,-129.6399 359.5767,-137.4654 362.8978,-131.3034\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140268165557264&#45;&gt;140268166613776 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140268165557264-&gt;140268166613776</title>\n",
       "<path d=\"M198.3976,-166.3799C216.7602,-156.4832 238.3653,-144.8388 257.6178,-134.4625\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"259.4233,-137.4654 266.5656,-129.6399 256.1022,-131.3034 259.4233,-137.4654\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140268568909392 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140268568909392</title>\n",
       "<polygon fill=\"none\" points=\"199.5,-.5 199.5,-46.5 419.5,-46.5 419.5,-.5 199.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"245\" y=\"-19.8\">dense: Dense</text>\n",
       "<polyline fill=\"none\" points=\"290.5,-.5 290.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"290.5,-23.5 346.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"346.5,-.5 346.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"383\" y=\"-31.3\">(?, ?, 256)</text>\n",
       "<polyline fill=\"none\" points=\"346.5,-23.5 419.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"383\" y=\"-8.3\">(?, ?, 105)</text>\n",
       "</g>\n",
       "<!-- 140268166613776&#45;&gt;140268568909392 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140268166613776-&gt;140268568909392</title>\n",
       "<path d=\"M309.5,-83.3799C309.5,-75.1745 309.5,-65.7679 309.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"313.0001,-56.784 309.5,-46.784 306.0001,-56.784 313.0001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True, dpi=60).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaddce9",
   "metadata": {},
   "source": [
    "# seq2seq 기계 번역기 동작시키기  \n",
    "  \n",
    "seq2seq는 훈련과 test할때의 동작이 다름  \n",
    "1. 번역하고자 하는 입력 문장이 인코더에 들어가서 은닉상태와 셀 상태를 얻음  \n",
    "2. 상태와 <SOS>에 해당하는 '\\t'를 디코더로 보냄  \n",
    "3. 디코더가 <EOS>에 해당하는 '\\n'이 나올때까지 다음 문자를 예측하는 행동을 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cf03b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder model\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "# 학습할때 썼던 inputs과 outputs을 테스트때도 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ffb07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder model #훈련과정과 다른 부분이 decoder 부분이므로 encoder model과 달리 training 때 했던걸 재활용 불가함\n",
    "\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "#위의 두개는 encoder_model의 encoder_states가 넘어갈거기 때문에 차원이 256인거\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "# decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "# 다음 단어를 예측하기 위해서 초기상태(initial_state)를 이전 시점의 상태로 사용\n",
    "# 이는 뒤의 함수 decode_sequence()에서 구현\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
    "                      outputs=[decoder_outputs] + decoder_states)\n",
    "# [[a,b,c]] + [[d,e]] = [[a,b,c],[d,e]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbb8f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [1,2,3,4]\n",
    "# b = [1,2,3,4]\n",
    "# c = a + b\n",
    "# print(c)\n",
    "\n",
    "# aa = [[1,2,3,4]]\n",
    "# bb = [[1,2,3,4]]\n",
    "# cc= aa + bb\n",
    "# print(cc)\n",
    "\n",
    "# dd = aa + a\n",
    "# print(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb233d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index로부터 단어를 얻을 수 있음\n",
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "#src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "# (a, 3)\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2ae1760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # inputs = encoder_inputs\n",
    "    # encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "    # outputs = encoder_states\n",
    "    # encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    #stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이전 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq]+states_value)\n",
    "        \n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
    "        # np.argmax 최대값의 index를 반환\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "        \n",
    "        #현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        #<eos>에 도달하거나 최대 길이를 넘으면 중단\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "            \n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1\n",
    "        \n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d56cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "# target_seq[0, 0, tar_to_index['\\t']] = 1\n",
    "# print(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f721747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: I see.\n",
      "정답 문장:  Aha. \n",
      "번역기가 번역한 문장:  Je crois. \n",
      "-----------------------------------\n",
      "입력 문장: Hug me.\n",
      "정답 문장:  Serrez-moi dans vos bras ! \n",
      "번역기가 번역한 문장:  Serrez-moi dans votre connaissain. \n",
      "-----------------------------------\n",
      "입력 문장: Hold it!\n",
      "정답 문장:  Restez où vous êtes ! \n",
      "번역기가 번역한 문장:  Ne bouge pas. \n",
      "-----------------------------------\n",
      "입력 문장: I crashed.\n",
      "정답 문장:  Je me suis écrasée. \n",
      "번역기가 번역한 문장:  Je me suis amusé. \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [50, 100, 300, 1001]:\n",
    "    input_seq = encoder_input[seq_index:seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1])\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24803aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I see.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lines.src[50])\n",
    "encoder_input[50:51].shape\n",
    "encoder_input[50:51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f419ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
